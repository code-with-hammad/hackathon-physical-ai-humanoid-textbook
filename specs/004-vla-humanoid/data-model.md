# Data Model for Module 4: Vision-Language-Action (VLA)

This document outlines the key entities and their conceptual relationships within the "Vision-Language-Action (VLA)" module, focusing on data flow between voice input, LLM planning, and robot execution.

## Entities

### 1. Voice Command
- **Description**: Spoken natural language instructions provided by a human user.
- **Key Attributes**:
    - `audio_data`: Raw audio stream containing the command.
    - `timestamp`: Time of command issuance.
- **Relationships**: Input to `OpenAI Whisper`.

### 2. OpenAI Whisper
- **Description**: A speech-to-text model responsible for transcribing `Voice Command` audio into text.
- **Key Attributes**:
    - `input_audio`: `audio_data` from `Voice Command`.
    - `output_text`: Transcribed text of the voice command.
- **Relationships**: Consumes `Voice Command`. Produces `Transcribed Text`.

### 3. Transcribed Text
- **Description**: The textual representation of the `Voice Command`, produced by `OpenAI Whisper`.
- **Key Attributes**:
    - `text_content`: String of the transcribed command.
    - `confidence_score`: Confidence level of the transcription.
- **Relationships**: Input to `Action Mapper` and `Large Language Model (LLM)`.

### 4. Action Mapper
- **Description**: A component that translates `Transcribed Text` into `Robot Action` if the command is direct.
- **Key Attributes**:
    - `input_text`: `Transcribed Text`.
    - `action_dictionary`: Predefined mapping from keywords/phrases to `Robot Action` types.
    - `output_robot_action`: If direct mapping found, produces a `Robot Action`.
- **Relationships**: Consumes `Transcribed Text`. Produces `Robot Action`.

### 5. Large Language Model (LLM)
- **Description**: An AI model used for cognitive planning, translating high-level natural language goals into `Cognitive Plan`s.
- **Key Attributes**:
    - `input_goal`: High-level natural language goal from `Transcribed Text`.
    - `input_perception_feedback`: `Perception Feedback` from `Humanoid Robot`.
    - `system_prompt`: Context and instructions for the LLM.
    - `tools/functions_available`: Descriptions of `ROS 2 Executable Action`s the robot can perform.
    - `output_cognitive_plan`: Sequence of high-level actions.
- **Relationships**: Consumes `Transcribed Text` and `Perception Feedback`. Produces `Cognitive Plan`.

### 6. Cognitive Plan
- **Description**: A structured sequence of high-level actions generated by the `LLM` to achieve a given goal.
- **Key Attributes**:
    - `sequence_of_actions`: Ordered list of `Robot Action`s or higher-level primitives.
    - `status`: (e.g., `pending`, `in_progress`, `completed`, `failed`).
- **Relationships**: Consumes `Robot Action`s. Orchestrates `ROS 2 Executable Action`s.

### 7. Robot Action
- **Description**: A discrete, executable command that a `Humanoid Robot` can perform.
- **Key Attributes**:
    - `action_type`: (e.g., `move_forward`, `turn_left`, `grasp_object`, `navigate_to_pose`).
    - `parameters`: Associated values (e.g., distance, angle, target object ID).
- **Relationships**: Executed by `Humanoid Robot` via `ROS 2 Executable Action`. Can be direct output of `Action Mapper` or part of `Cognitive Plan`.

### 8. ROS 2 Executable Action
- **Description**: A low-level ROS 2 command (e.g., topic publish, service call, action goal) that directly controls a robot's capabilities.
- **Key Attributes**:
    - `ros2_interface_type`: (e.g., `topic`, `service`, `action`).
    - `ros2_message`: Specific ROS 2 message content.
    - `target_node/component`: ROS 2 entity that will execute the command.
- **Relationships**: Translates `Robot Action` into physical robot commands.

### 9. Humanoid Robot
- **Description**: The simulated bipedal robot executing the actions and providing perception.
- **Key Attributes**:
    - `current_pose`: Position and orientation.
    - `joint_states`: Angles and velocities of joints.
    - `sensor_data`: Raw sensor readings (e.g., camera images, depth, LiDAR, IMU).
- **Relationships**: Executes `ROS 2 Executable Action`s. Provides `Perception Feedback`.

### 10. Perception Feedback
- **Description**: Information from `Humanoid Robot` sensors (e.g., camera, LiDAR) about the environment and objects.
- **Key Attributes**:
    - `object_detections`: List of detected objects, their types, and poses.
    - `environment_map`: Occupancy grid or point cloud representation of the environment.
    - `robot_status`: Internal state and health of the robot.
- **Relationships**: Input to `LLM` for contextual planning.

## Data Flow

`Voice Command` --> `OpenAI Whisper` --> `Transcribed Text` --> (`Action Mapper` or `LLM`)
    - If `Action Mapper`: `Transcribed Text` --> `Robot Action` --> `ROS 2 Executable Action` --> `Humanoid Robot`
    - If `LLM`: `Transcribed Text` + `Perception Feedback` --> `LLM` --> `Cognitive Plan` (sequence of `Robot Action`s) --> `ROS 2 Executable Action` --> `Humanoid Robot`

## Validation Rules

- **Voice Command Transcription**: High accuracy (e.g., >95%) for clear speech.
- **Action Mapping**: Robust mapping for common robot actions.
- **LLM Planning**: Plans must be logically consistent, executable, and adhere to robot capabilities and safety.
- **ROS 2 Action Execution**: Commands must be correctly interpreted and executed by the `Humanoid Robot`.
- **Perception Feedback**: Timely and accurate to inform LLM planning.
